{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test log softmax on Movielens1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pdb\n",
    "from heapq import heappush, heappop\n",
    "from auxiliary import ScaledEmbedding, ZeroEmbedding\n",
    "import evaluation\n",
    "import data_loader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from cpt import PT_LogSoftmax\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params = dict()\n",
    "params['lr'] = 1e-4\n",
    "params['batch_size'] = 256\n",
    "params['epoch_limit'] = 50\n",
    "params['w_decay'] = 1e-1\n",
    "params['negNum_test'] = 100\n",
    "params['epsilon'] = 1e-4\n",
    "params['negNum_train'] = 2\n",
    "params['l_size'] = 16\n",
    "params['train_device'] = 'cuda:2'\n",
    "params['test_device'] = 'cuda:2'\n",
    "params['test_per_train'] = 10\n",
    "\n",
    "Precision@: {1:0.25231788079470197; 5: 0.09933774834437077; 10: 0.06115894039735356}\n",
    "Recall@: {1:0.25231788079470197; 5: 0.4966887417218543; 10: 0.6115894039735099}\n",
    "F1@: {1:0.25231788079470197; 5: 0.16556291390728464; 10: 0.11119807344973333}\n",
    "NDCG@: {1:0.25231788079470197; 5: 0.4307096069509847; 10: 0.4701980019108464}\n",
    "AUC:0.14624999999999716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['lr'] = 1e-4\n",
    "params['batch_size'] = 256\n",
    "params['epoch_limit'] = 50\n",
    "params['w_decay'] = 1e-1\n",
    "params['negNum_test'] = 100\n",
    "params['epsilon'] = 1e-4\n",
    "params['negNum_train'] = 2\n",
    "params['l_size'] = 16\n",
    "params['train_device'] = 'cuda:2'\n",
    "params['test_device'] = 'cuda:2'\n",
    "params['test_per_train'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Movielens1M'\n",
    "\n",
    "train, test = data_loader.read_data(category)\n",
    "userNum, itemNum = data_loader.get_datasize(category)\n",
    "frequency = data_loader.get_distribution(category)\n",
    "distribution = data_loader.approx_Gaussian(frequency)\n",
    "item_price = data_loader.get_price(category)\n",
    "\n",
    "trainset = data_loader.TransactionData(train, userNum, itemNum, distribution)\n",
    "trainset.set_negN(params['negNum_train'])\n",
    "trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "testset = data_loader.UserTransactionData(test, userNum, itemNum, trainset.userHist)\n",
    "testset.set_negN(params['negNum_test'])\n",
    "testLoader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT(nn.Module):\n",
    "    def __init__(self, userLen, itemLen, distribution, params, item_price):\n",
    "        super(PT, self).__init__()\n",
    "        self.userNum = userLen\n",
    "        self.itemNum = itemLen\n",
    "        self.params = params\n",
    "\n",
    "        if 'gpu' in params and params['gpu'] == True:\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        l_size = params['l_size']\n",
    "        self.distribution = torch.FloatTensor(distribution).to(self.device)\n",
    "        self.item_price = torch.FloatTensor(item_price).to(self.device)\n",
    "        \n",
    "        self.globalBias_g = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_g.weight.data += 0.5\n",
    "        self.globalBias_g.weight.requires_grad = False\n",
    "        self.userBias_g = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_g = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_d = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_d.weight.data += 0.5\n",
    "        self.globalBias_d.weight.requires_grad = False\n",
    "        self.userBias_d = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_d = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_a = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_a.weight.requires_grad = False\n",
    "        self.userBias_a = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_a = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.globalBias_b = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_b.weight.requires_grad = False\n",
    "        self.userBias_b = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_b = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.globalBias_l = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_l.weight.requires_grad = False\n",
    "        self.userBias_l = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_l = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "        self.reference_point = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.reference_point.weight.data = torch.ones_like(self.reference_point.weight.data)*1.5\n",
    "#         self.reference_point.weight.requires_grad=False\n",
    "        self.to(self.device)  \n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        distribution = self.distribution[items].to(self.device)\n",
    "        reference_point = self.reference_point(users)\n",
    "        price = self.item_price[items].view(-1,1).expand(users.shape[0],5).to(self.device)\n",
    "\n",
    "        # calculate value\n",
    "        globalBias_a = self.globalBias_a(torch.tensor(0).to(self.device))\n",
    "        userBias_a = self.userBias_a(users)\n",
    "        itemBias_a = self.itemBias_a(items)\n",
    "        userEmbed_a = self.userEmbed_a(users)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        globalBias_b = self.globalBias_b(torch.tensor(0).to(self.device))\n",
    "        userBias_b = self.userBias_b(users)\n",
    "        itemBias_b = self.itemBias_b(items)\n",
    "        userEmbed_b = self.userEmbed_b(users)\n",
    "        itemEmbed_b = self.itemEmbed_b(items)\n",
    "        \n",
    "        globalBias_l = self.globalBias_l(torch.tensor(0).to(self.device))\n",
    "        userBias_l = self.userBias_l(users)\n",
    "        itemBias_l = self.itemBias_l(items)\n",
    "        userEmbed_l = self.userEmbed_l(users)\n",
    "        itemEmbed_l = self.itemEmbed_l(items)\n",
    "\n",
    "        alpha = globalBias_a + userBias_a + itemBias_a + torch.mul(userEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        beta = globalBias_b + userBias_b + itemBias_b + torch.mul(userEmbed_b, itemEmbed_b).sum(1).view(-1, 1)\n",
    "        lamda = globalBias_l + userBias_l + itemBias_l + torch.mul(userEmbed_l, itemEmbed_l).sum(1).view(-1, 1)\n",
    "\n",
    "        rating = torch.tensor([1., 2., 3., 4., 5.]).expand(users.shape[0], 5).to(self.device)\n",
    "        x = torch.tanh(rating - reference_point)\n",
    "        x_binary_pos = torch.gt(x, torch.FloatTensor([0]).to(self.device)).to(torch.float)\n",
    "        x_binary_neg = torch.ones_like(x).to(self.device) - x_binary_pos\n",
    "        \n",
    "#         x_ = torch.mul(price,torch.abs(x))\n",
    "        x_ = torch.abs(x)\n",
    "        v_exp = torch.mul(alpha, x_binary_pos) + torch.mul(beta, x_binary_neg)\n",
    "        v = x_.pow(v_exp)\n",
    "        v_coef = torch.mul(lamda, x_binary_pos) - x_binary_neg\n",
    "        value = torch.mul(v,v_coef).to(self.device)        \n",
    "\n",
    "\n",
    "        # calculate weight\n",
    "        globalBias_g = self.globalBias_g(torch.tensor(0).to(self.device))\n",
    "        userBias_g = self.userBias_g(users)\n",
    "        itemBias_g = self.itemBias_g(items)\n",
    "        userEmbed_g = self.userEmbed_g(users)\n",
    "        itemEmbed_g = self.itemEmbed_g(items)\n",
    "\n",
    "        globalBias_d = self.globalBias_d(torch.tensor(0).to(self.device))\n",
    "        userBias_d = self.userBias_d(users)\n",
    "        itemBias_d = self.itemBias_d(items)\n",
    "        userEmbed_d = self.userEmbed_d(users)\n",
    "        itemEmbed_d = self.itemEmbed_d(items)\n",
    "\n",
    "        gamma = globalBias_g + userBias_g + itemBias_g + torch.mul(userEmbed_g, itemEmbed_g).sum(1).view(-1, 1)\n",
    "        delta = globalBias_d + userBias_d + itemBias_d + torch.mul(userEmbed_d, itemEmbed_d).sum(1).view(-1, 1)\n",
    "\n",
    "        gamma_ = gamma.expand(users.shape[0],5)\n",
    "        delta_ = delta.expand(users.shape[0],5)\n",
    "        w_exp = torch.mul(x_binary_pos,gamma_) + torch.mul(x_binary_neg,delta_)\n",
    "        \n",
    "        w_nominator = distribution.pow(w_exp)\n",
    "        w_denominator = (distribution.pow(w_exp)+(torch.ones_like(distribution).to(self.device)-distribution).pow(w_exp)).pow(1/w_exp)\n",
    "        weight = torch.div(w_nominator, w_denominator)\n",
    "        \n",
    "\n",
    "#         self.userBias_g.weight.register_hook(self.save_grad('userBias_g'))\n",
    "#         self.itemBias_g.weight.register_hook(self.save_grad('itemBias_g'))\n",
    "#         self.userEmbed_g.weight.register_hook(self.save_grad('userEmbed_g'))\n",
    "#         self.itemEmbed_g.weight.register_hook(self.save_grad('itemEmbed_g'))\n",
    "        return torch.mul(weight, value).sum(1)\n",
    "\n",
    "    def loss(self, users, items, negItems):\n",
    "        nusers = users.view(-1, 1).to(self.device)\n",
    "        nusers = nusers.expand(nusers.shape[0], self.params['negNum_train']).reshape(-1).to(self.device)\n",
    "\n",
    "        pOut = self.forward(users, items).view(-1, 1)#.expand(users.shape[0], self.params['negNum_train']).reshape(-1, 1)\n",
    "        nOut = self.forward(nusers, negItems).reshape(-1, self.params['negNum_train'])\n",
    "        Out = torch.cat((pOut,nOut),dim=1)\n",
    "        \n",
    "#         print(Out.shape)\n",
    "#         print(nOut.shape)\n",
    "#         input()\n",
    "        criterion = nn.LogSoftmax(dim=1)\n",
    "        res = criterion(Out)[:,0]\n",
    "        loss = torch.mean(res)\n",
    "        return -loss\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.grads\n",
    "\n",
    "    def save_grad(self, name):\n",
    "        def hook(grad):\n",
    "            self.grads[name] = grad\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch  1  training...\n",
      "processed: 1: 100%|██████████| 988129/988129 [02:24<00:00, 6822.19it/s]\n",
      "epoch loss tensor(4120.8696, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  2  training...\n",
      "processed: 2: 100%|██████████| 988129/988129 [02:22<00:00, 6923.42it/s]\n",
      "epoch loss tensor(4190.0693, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  3  training...\n",
      "processed: 3: 100%|██████████| 988129/988129 [02:24<00:00, 6829.31it/s]\n",
      "epoch loss tensor(4227.5044, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  4  training...\n",
      "processed: 4: 100%|██████████| 988129/988129 [02:28<00:00, 6673.82it/s]\n",
      "epoch loss tensor(4227.5112, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  5  training...\n",
      "processed: 5: 100%|██████████| 988129/988129 [02:28<00:00, 6660.85it/s]\n",
      "epoch loss tensor(4227.5078, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  6  training...\n",
      "processed: 6: 100%|██████████| 988129/988129 [02:29<00:00, 6631.52it/s]\n",
      "epoch loss tensor(4227.4956, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  7  training...\n",
      "processed: 7: 100%|██████████| 988129/988129 [02:28<00:00, 6658.13it/s]\n",
      "epoch loss tensor(4227.5034, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  8  training...\n",
      "processed: 8: 100%|██████████| 988129/988129 [02:28<00:00, 6650.88it/s]\n",
      "epoch loss tensor(4227.4912, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  9  training...\n",
      "processed: 9: 100%|██████████| 988129/988129 [02:29<00:00, 6595.72it/s]\n",
      "epoch loss tensor(4227.4946, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  10  training...\n",
      "processed: 10: 100%|██████████| 988129/988129 [02:27<00:00, 6677.13it/s]\n",
      "epoch loss tensor(4227.5020, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  11  training...\n",
      "processed: 11: 100%|██████████| 988129/988129 [02:25<00:00, 6794.06it/s]\n",
      "epoch loss tensor(4227.5122, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  12  training...\n",
      "processed: 12: 100%|██████████| 988129/988129 [02:29<00:00, 6629.96it/s]\n",
      "epoch loss tensor(4227.4976, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  13  training...\n",
      "processed: 13: 100%|██████████| 988129/988129 [02:27<00:00, 6678.10it/s]\n",
      "epoch loss tensor(4227.4878, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  14  training...\n",
      "processed: 14: 100%|██████████| 988129/988129 [02:28<00:00, 6639.57it/s]\n",
      "epoch loss tensor(4227.4902, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  15  training...\n",
      "processed: 15: 100%|██████████| 988129/988129 [02:28<00:00, 6652.84it/s]\n",
      "epoch loss tensor(4227.5010, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  16  training...\n",
      "processed: 16: 100%|██████████| 988129/988129 [02:29<00:00, 6628.61it/s]\n",
      "epoch loss tensor(4227.5063, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  17  training...\n",
      "processed: 17: 100%|██████████| 988129/988129 [02:26<00:00, 6745.72it/s]\n",
      "epoch loss tensor(4227.5034, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  18  training...\n",
      "processed: 18: 100%|██████████| 988129/988129 [02:25<00:00, 6775.66it/s]\n",
      "epoch loss tensor(4227.5005, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  19  training...\n",
      "processed: 19: 100%|██████████| 988129/988129 [02:19<00:00, 7083.71it/s]\n",
      "epoch loss tensor(4227.5000, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  20  training...\n",
      "processed: 20: 100%|██████████| 988129/988129 [02:18<00:00, 7116.27it/s]\n",
      "epoch loss tensor(4227.5068, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  21  training...\n",
      "processed: 21: 100%|██████████| 988129/988129 [02:16<00:00, 7244.34it/s]\n",
      "epoch loss tensor(4227.4805, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  22  training...\n",
      "processed: 22: 100%|██████████| 988129/988129 [02:15<00:00, 7288.37it/s]\n",
      "epoch loss tensor(4227.4858, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  23  training...\n",
      "processed: 23: 100%|██████████| 988129/988129 [02:12<00:00, 7433.09it/s]\n",
      "epoch loss tensor(4227.4863, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  24  training...\n",
      "processed: 24: 100%|██████████| 988129/988129 [02:17<00:00, 7170.81it/s]\n",
      "epoch loss tensor(4227.4922, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  25  training...\n",
      "processed: 25: 100%|██████████| 988129/988129 [02:14<00:00, 7319.80it/s]\n",
      "epoch loss tensor(4227.5020, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  26  training...\n",
      "processed: 26: 100%|██████████| 988129/988129 [02:18<00:00, 7120.01it/s]\n",
      "epoch loss tensor(4227.5029, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  27  training...\n",
      "processed: 27: 100%|██████████| 988129/988129 [02:03<00:00, 8014.17it/s]\n",
      "epoch loss tensor(4227.4917, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  28  training...\n",
      "processed: 28: 100%|██████████| 988129/988129 [02:01<00:00, 8126.46it/s]\n",
      "epoch loss tensor(4227.4878, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  29  training...\n",
      "processed: 29: 100%|██████████| 988129/988129 [02:02<00:00, 8038.65it/s]\n",
      "epoch loss tensor(4227.5000, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  30  training...\n",
      "processed: 30: 100%|██████████| 988129/988129 [02:03<00:00, 7983.16it/s]\n",
      "epoch loss tensor(4227.5029, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  31  training...\n",
      "processed: 31: 100%|██████████| 988129/988129 [02:05<00:00, 7892.10it/s]\n",
      "epoch loss tensor(4227.4863, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  32  training...\n",
      "processed: 32: 100%|██████████| 988129/988129 [02:05<00:00, 7879.96it/s]\n",
      "epoch loss tensor(4227.4805, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  33  training...\n",
      "processed: 33: 100%|██████████| 988129/988129 [02:04<00:00, 7937.66it/s]\n",
      "epoch loss tensor(4227.4839, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  34  training...\n",
      "processed: 34: 100%|██████████| 988129/988129 [02:04<00:00, 7933.05it/s]\n",
      "epoch loss tensor(4227.4775, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  35  training...\n",
      "processed: 35: 100%|██████████| 988129/988129 [02:04<00:00, 7920.93it/s]\n",
      "epoch loss tensor(4227.4961, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  36  training...\n",
      "processed: 36: 100%|██████████| 988129/988129 [02:04<00:00, 7939.91it/s]\n",
      "epoch loss tensor(4227.4941, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  37  training...\n",
      "processed: 37: 100%|██████████| 988129/988129 [02:03<00:00, 7983.35it/s]\n",
      "epoch loss tensor(4227.4858, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  38  training...\n",
      "processed: 38: 100%|██████████| 988129/988129 [02:04<00:00, 7966.26it/s]\n",
      "epoch loss tensor(4227.5044, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  39  training...\n",
      "processed: 39: 100%|██████████| 988129/988129 [02:04<00:00, 7954.13it/s]\n",
      "epoch loss tensor(4227.5298, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  40  training...\n",
      "processed: 40: 100%|██████████| 988129/988129 [02:04<00:00, 7958.48it/s]\n",
      "epoch loss tensor(4227.5068, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  41  training...\n",
      "processed: 41: 100%|██████████| 988129/988129 [02:05<00:00, 7863.31it/s]\n",
      "epoch loss tensor(4227.5303, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  42  training...\n",
      "processed: 42: 100%|██████████| 988129/988129 [02:03<00:00, 7989.14it/s]\n",
      "epoch loss tensor(4227.5171, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  43  training...\n",
      "processed: 43: 100%|██████████| 988129/988129 [02:04<00:00, 7909.27it/s]\n",
      "epoch loss tensor(4227.5000, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  44  training...\n",
      "processed: 44: 100%|██████████| 988129/988129 [01:59<00:00, 8270.10it/s]\n",
      "epoch loss tensor(4227.4946, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  45  training...\n",
      "processed: 45: 100%|██████████| 988129/988129 [01:58<00:00, 8366.03it/s]\n",
      "epoch loss tensor(4227.4741, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  46  training...\n",
      "processed: 46: 100%|██████████| 988129/988129 [01:57<00:00, 8394.55it/s]\n",
      "epoch loss tensor(4227.5293, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  47  training...\n",
      "processed: 47: 100%|██████████| 988129/988129 [01:56<00:00, 8471.23it/s]\n",
      "epoch loss tensor(4227.5044, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  48  training...\n",
      "processed: 48: 100%|██████████| 988129/988129 [02:03<00:00, 7977.97it/s]\n",
      "epoch loss tensor(4227.4951, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  49  training...\n",
      "processed: 49: 100%|██████████| 988129/988129 [02:05<00:00, 7863.20it/s]\n",
      "epoch loss tensor(4227.5044, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "Epoch  50  training...\n",
      "processed: 50: 100%|██████████| 988129/988129 [02:05<00:00, 7864.01it/s]\n",
      "epoch loss tensor(4227.4829, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "starting test...\n",
      "100%|██████████| 6040/6040 [01:11<00:00, 84.97it/s]\n",
      "length of score_dict 6040\n",
      "\tPrecision@: {1:0.24155629139072848; 5: 0.09877483443708587; 10: 0.06094370860927404; 20: 0.03705298013245308}\n",
      "\tRecall@: {1:0.24155629139072848; 5: 0.49387417218543045; 10: 0.6094370860927152; 20: 0.7410596026490066}\n",
      "\tF1@: {1:0.24155629139072848; 5: 0.16462472406180984; 10: 0.11080674292595238; 20: 0.0705771050141961}\n",
      "\tNDCG@: {1:0.24155629139072848; 5: 0.42637746062141296; 10: 0.4663051616477742; 20: 0.5003282970653905}\n"
     ]
    }
   ],
   "source": [
    "model = PT_LogSoftmax(userLen=userNum, itemLen=itemNum, distribution=distribution, params=params, item_price=item_price)\n",
    "# print('initialization', model.state_dict())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "\n",
    "epoch = 0\n",
    "print('start training...')\n",
    "while epoch < params['epoch_limit']:\n",
    "    model.device = params['train_device']\n",
    "    model.to(model.device)\n",
    "\n",
    "    epoch += 1\n",
    "    print('Epoch ', str(epoch), ' training...')\n",
    "    L = len(trainLoader.dataset)\n",
    "    pbar = tqdm(total = L, file=sys.stdout)\n",
    "    pbar.set_description('processed: %d' % epoch)\n",
    "    for i, batchData in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        users = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        items = torch.LongTensor(batchData['item']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).reshape(-1).to(model.device)\n",
    "\n",
    "        batch_loss = model.loss(users, items, negItems)\n",
    "        batch_loss.backward()\n",
    "        grads = model.get_grads()\n",
    "        \n",
    "#         print('userBias_g:',grads['userBias_g'])\n",
    "#         print('itemBias_g:',grads['itemBias_g'])\n",
    "#         print('userEmbed_g:',grads['userEmbed_g'])\n",
    "#         print('itemEmbed_g:',grads['itemEmbed_g'])\n",
    "#         input()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i == 0:\n",
    "            total_loss = batch_loss.clone()\n",
    "        else:\n",
    "            total_loss += batch_loss.clone()\n",
    "        pbar.update(users.shape[0])\n",
    "    pbar.close()\n",
    "    # torch.save(model, 'pt.pt')\n",
    "    print('epoch loss', total_loss)\n",
    "#     print(model.state_dict())\n",
    "\n",
    "    if epoch % params['test_per_train'] == 0:\n",
    "        print('starting val...')\n",
    "        model.device = params['test_device']\n",
    "        model.to(model.device)\n",
    "        L = len(testLoader.dataset)\n",
    "        pbar = tqdm(total=L, file=sys.stdout)\n",
    "        with torch.no_grad():\n",
    "            scoreDict = dict()\n",
    "            for i, batchData in enumerate(testLoader):\n",
    "#                 if np.random.random() < 0.98:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "#                 if i%50 != 0:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "                user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "                posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "                negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "                items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "                users = user.expand(items.shape[0])\n",
    "\n",
    "                score = model.forward(users, items)\n",
    "                scoreHeap = list()\n",
    "                for j in range(score.shape[0]):\n",
    "                    gt = False\n",
    "                    if j < posItems.shape[1]:\n",
    "                        gt = True\n",
    "\n",
    "                    heappush(scoreHeap, (1-score[j].cpu().numpy(), (0+items[j].cpu().numpy(), gt)))\n",
    "                scores = list()\n",
    "                candidate = len(scoreHeap)\n",
    "                for k in range(candidate):\n",
    "                    scores.append(heappop(scoreHeap))\n",
    "                pbar.update(1)\n",
    "                scoreDict[user[0]] = (scores, posItems.shape[1])\n",
    "        pbar.close()\n",
    "        testResult = evaluation.ranking_performance(scoreDict, 100)\n",
    "#         with open('./results/'+category+'/'+category+'_PT_valResult_'+str(epoch)+'.json', 'w') as outfile:\n",
    "#             json.dump(testResult, outfile)\n",
    "        \n",
    "print('starting test...')\n",
    "model.device = params['test_device']\n",
    "model.to(model.device)\n",
    "L = len(testLoader.dataset)\n",
    "pbar = tqdm(total=L, file=sys.stdout)\n",
    "alpha = []\n",
    "beta = []\n",
    "lamda = []\n",
    "gamma = []\n",
    "delta = []\n",
    "ref = []\n",
    "with torch.no_grad():\n",
    "    scoreDict = dict()\n",
    "    for i, batchData in enumerate(testLoader):\n",
    "        user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "        items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "        users = user.expand(items.shape[0])\n",
    "        \n",
    "        [a,b,l,g,d,r] = model.get_paras(users,items)\n",
    "    \n",
    "        \n",
    "        alpha.append(a.cpu().numpy())\n",
    "        beta.append(b.cpu().numpy())\n",
    "        lamda.append(l.cpu().numpy())\n",
    "        gamma.append(g.cpu().numpy())\n",
    "        delta.append(d.cpu().numpy())\n",
    "        ref.append(r.cpu().numpy())\n",
    "        \n",
    "        score = model.forward(users, items)\n",
    "        scoreHeap = list()\n",
    "        for j in range(score.shape[0]):\n",
    "            gt = False\n",
    "            if j < posItems.shape[1]:\n",
    "                gt = True\n",
    "\n",
    "            heappush(scoreHeap, (1-score[j].cpu().numpy(), (0+items[j].cpu().numpy(), gt)))\n",
    "        scores = list()\n",
    "        candidate = len(scoreHeap)\n",
    "        for k in range(candidate):\n",
    "            scores.append(heappop(scoreHeap))\n",
    "        pbar.update(1)\n",
    "        scoreDict[int(user[0].cpu().numpy())] = (scores, posItems.shape[1])\n",
    "pbar.close()\n",
    "testResult = evaluation.ranking_performance(scoreDict, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/'+category+'/'+category+'_PT_testResult.json', 'w') as outfile:\n",
    "    json.dump(testResult, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = np.array([float(j) for i in alpha for j in i])\n",
    "b_ = np.array([float(j) for i in beta for j in i])\n",
    "g_ = np.array([float(j) for i in gamma for j in i])\n",
    "d_ = np.array([float(j) for i in delta for j in i])\n",
    "l_ = np.array([float(j) for i in lamda for j in i])\n",
    "r_ = np.array([float(j) for i in ref for j in i])\n",
    "\n",
    "np.save('./results/'+category+'/'+category+'_PT_alpha.npy',a_)\n",
    "np.save('./results/'+category+'/'+category+'_PT_beta.npy',b_)\n",
    "np.save('./results/'+category+'/'+category+'_PT_gamma.npy',g_)\n",
    "np.save('./results/'+category+'/'+category+'_PT_delta.npy',d_)\n",
    "np.save('./results/'+category+'/'+category+'_PT_lamda.npy',l_)\n",
    "np.save('./results/'+category+'/'+category+'_PT_reference.npy',r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result = plt.hist(r_,bins=20,range=(-5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
